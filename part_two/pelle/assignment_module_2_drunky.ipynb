{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MNBgGYg_lpVN"
      },
      "source": [
        "## Assignment Module 2: Product Classification\n",
        "\n",
        "The goal of this assignment is to implement a neural network that classifies smartphone pictures of products found in grocery stores. The assignment will be divided into two parts: first, you will be asked to implement from scratch your own neural network for image classification; then, you will fine-tune a pretrained network provided by PyTorch.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dVTQUJ4uYH1w"
      },
      "source": [
        "### Preliminaries: the dataset\n",
        "\n",
        "The dataset you will be using contains natural images of products taken with a smartphone camera in different grocery stores:\n",
        "\n",
        "<p align=\"center\">\n",
        "  <img src=\"https://github.com/marcusklasson/GroceryStoreDataset/raw/master/sample_images/natural/Granny-Smith.jpg\" width=\"150\">\n",
        "  <img src=\"https://github.com/marcusklasson/GroceryStoreDataset/raw/master/sample_images/natural/Pink-Lady.jpg\" width=\"150\">\n",
        "  <img src=\"https://github.com/marcusklasson/GroceryStoreDataset/raw/master/sample_images/natural/Lemon.jpg\" width=\"150\">\n",
        "  <img src=\"https://github.com/marcusklasson/GroceryStoreDataset/raw/master/sample_images/natural/Banana.jpg\" width=\"150\">\n",
        "  <img src=\"https://github.com/marcusklasson/GroceryStoreDataset/raw/master/sample_images/natural/Vine-Tomato.jpg\" width=\"150\">\n",
        "</p>\n",
        "<p align=\"center\">\n",
        "  <img src=\"https://github.com/marcusklasson/GroceryStoreDataset/raw/master/sample_images/natural/Yellow-Onion.jpg\" width=\"150\">\n",
        "  <img src=\"https://github.com/marcusklasson/GroceryStoreDataset/raw/master/sample_images/natural/Green-Bell-Pepper.jpg\" width=\"150\">\n",
        "  <img src=\"https://github.com/marcusklasson/GroceryStoreDataset/raw/master/sample_images/natural/Arla-Standard-Milk.jpg\" width=\"150\">\n",
        "  <img src=\"https://github.com/marcusklasson/GroceryStoreDataset/raw/master/sample_images/natural/Oatly-Natural-Oatghurt.jpg\" width=\"150\">\n",
        "  <img src=\"https://github.com/marcusklasson/GroceryStoreDataset/raw/master/sample_images/natural/Alpro-Fresh-Soy-Milk.jpg\" width=\"150\">\n",
        "</p>\n",
        "\n",
        "The products belong to the following 43 classes:\n",
        "```\n",
        "0.  Apple\n",
        "1.  Avocado\n",
        "2.  Banana\n",
        "3.  Kiwi\n",
        "4.  Lemon\n",
        "5.  Lime\n",
        "6.  Mango\n",
        "7.  Melon\n",
        "8.  Nectarine\n",
        "9.  Orange\n",
        "10. Papaya\n",
        "11. Passion-Fruit\n",
        "12. Peach\n",
        "13. Pear\n",
        "14. Pineapple\n",
        "15. Plum\n",
        "16. Pomegranate\n",
        "17. Red-Grapefruit\n",
        "18. Satsumas\n",
        "19. Juice\n",
        "20. Milk\n",
        "21. Oatghurt\n",
        "22. Oat-Milk\n",
        "23. Sour-Cream\n",
        "24. Sour-Milk\n",
        "25. Soyghurt\n",
        "26. Soy-Milk\n",
        "27. Yoghurt\n",
        "28. Asparagus\n",
        "29. Aubergine\n",
        "30. Cabbage\n",
        "31. Carrots\n",
        "32. Cucumber\n",
        "33. Garlic\n",
        "34. Ginger\n",
        "35. Leek\n",
        "36. Mushroom\n",
        "37. Onion\n",
        "38. Pepper\n",
        "39. Potato\n",
        "40. Red-Beet\n",
        "41. Tomato\n",
        "42. Zucchini\n",
        "```\n",
        "\n",
        "The dataset is split into training (`train`), validation (`val`), and test (`test`) set."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1pdrmJRnJPd8"
      },
      "source": [
        "The following code cells download the dataset and define a `torch.utils.data.Dataset` class to access it. This `Dataset` class will be the starting point of your assignment: use it in your own code and build everything else around it."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "POMX_3x-_bZI",
        "outputId": "d6ee77cb-b2dd-488d-e5b5-f0677d0b25ed"
      },
      "outputs": [],
      "source": [
        "try:\n",
        "    import google.colab\n",
        "    !git clone https://github.com/marcusklasson/GroceryStoreDataset.git\n",
        "except: pass"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "hiF8xGEYlsu8"
      },
      "outputs": [],
      "source": [
        "from pathlib import Path\n",
        "from PIL import Image\n",
        "from torch import Tensor\n",
        "from torch.utils.data import Dataset\n",
        "from typing import List, Tuple"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "jROSO2qVDxdD"
      },
      "outputs": [],
      "source": [
        "class GroceryStoreDataset(Dataset):\n",
        "\n",
        "    def __init__(self, split: str, transform=None) -> None:\n",
        "        super().__init__()\n",
        "\n",
        "        self.root = Path(\"GroceryStoreDataset/dataset\")\n",
        "        self.split = split\n",
        "        self.paths, self.labels = self.read_file()\n",
        "\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self) -> int:\n",
        "        return len(self.labels)\n",
        "\n",
        "    def __getitem__(self, idx) -> Tuple[Tensor, int]:\n",
        "        img = Image.open(self.root / self.paths[idx])\n",
        "        label = self.labels[idx]\n",
        "\n",
        "        if self.transform:\n",
        "            img = self.transform(img)\n",
        "\n",
        "        return img, label\n",
        "\n",
        "    def read_file(self) -> Tuple[List[str], List[int]]:\n",
        "        paths = []\n",
        "        labels = []\n",
        "\n",
        "        with open(self.root / f\"{self.split}.txt\") as f:\n",
        "            for line in f:\n",
        "                # path, fine-grained class, coarse-grained class\n",
        "                path, _, label = line.replace(\"\\n\", \"\").split(\", \")\n",
        "                paths.append(path), labels.append(int(label))\n",
        "\n",
        "        return paths, labels\n",
        "\n",
        "    def get_num_classes(self) -> int:\n",
        "        return max(self.labels) + 1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yBch3dpwNSsW"
      },
      "source": [
        "## Part 1: design your own network\n",
        "\n",
        "Your goal is to implement a convolutional neural network for image classification and train it on `GroceryStoreDataset`. You should consider yourselves satisfied once you obtain a classification accuracy on the **validation** split of **around 60%**. You are free to achieve that however you want, except for a few rules you must follow:\n",
        "\n",
        "- You **cannot** simply instantiate an off-the-self PyTorch network. Instead, you must construct your network as a composition of existing PyTorch layers. In more concrete terms, you can use e.g. `torch.nn.Linear`, but you **cannot** use e.g. `torchvision.models.alexnet`.\n",
        "\n",
        "- Justify every *design choice* you make. Design choices include network architecture, training hyperparameters, and, possibly, dataset preprocessing steps. You can either (i) start from the simplest convolutional network you can think of and add complexity one step at a time, while showing how each step gets you closer to the target ~60%, or (ii) start from a model that is already able to achieve the desired accuracy and show how, by removing some of its components, its performance drops (i.e. an *ablation study*). You can *show* your results/improvements however you want: training plots, console-printed values or tables, or whatever else your heart desires: the clearer, the better.\n",
        "\n",
        "Don't be too concerned with your network performance: the ~60% is just to give you an idea of when to stop. Keep in mind that a thoroughly justified model with lower accuracy will be rewarded **more** points than a poorly experimentally validated model with higher accuracy."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F5VsEPrKH8qH",
        "outputId": "e0e3dddb-6d22-411a-d7de-9b0bfed0fd5a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: torchsummary in c:\\users\\davide\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (1.5.1)\n"
          ]
        }
      ],
      "source": [
        "# ! python3 -m pip install --upgrade pip\n",
        "! pip install -q wandb\n",
        "! pip install -q torchmetrics\n",
        "! pip install torchsummary"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F6dJm-3pH8qJ"
      },
      "source": [
        "### Weights and Biases for following the net"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "2zmkulI0H8qJ"
      },
      "outputs": [],
      "source": [
        "import wandb\n",
        "\n",
        "WANDB_USER = \"davidecrociati\"\n",
        "WANDB_PROJECT = \"assignments part two\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IEaLcecbH8qK"
      },
      "source": [
        "### Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "QmeBcj2MH8qK"
      },
      "outputs": [],
      "source": [
        "# Standard library imports\n",
        "import csv\n",
        "import copy\n",
        "import random\n",
        "from pathlib import Path\n",
        "from typing import Any, Dict\n",
        "import os\n",
        "import json\n",
        "from datetime import datetime\n",
        "import time\n",
        "\n",
        "# Third-party library imports\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from PIL import Image\n",
        "from tqdm import tqdm\n",
        "from tqdm.notebook import tqdm\n",
        "\n",
        "# PyTorch imports\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.optim import AdamW\n",
        "from torch.optim.lr_scheduler import OneCycleLR, LambdaLR\n",
        "from torch.utils.data import DataLoader\n",
        "import torch.utils\n",
        "from torchmetrics.classification.accuracy import Accuracy\n",
        "from torchsummary import summary\n",
        "\n",
        "# Torchvision imports\n",
        "from torchvision import transforms as T\n",
        "from torchvision.models import resnet18, ResNet18_Weights"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "32kWjbqzH8qM"
      },
      "source": [
        "### Functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "sjbmdhzOH8qN"
      },
      "outputs": [],
      "source": [
        "def fix_random(seed: int) -> None:\n",
        "    \"\"\"Fix all the possible sources of randomness.\n",
        "\n",
        "    Args:\n",
        "        seed: the seed to use.\n",
        "    \"\"\"\n",
        "    np.random.seed(seed)\n",
        "    random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed(seed)\n",
        "\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "\n",
        "def extract_classes(csv_file_path: str) -> dict:\n",
        "    \"\"\"\n",
        "    Extract unique pairs of IDs and labels from a CSV file.\n",
        "\n",
        "    This function reads a CSV file, extracts the third and fourth columns,\n",
        "    and creates a dictionary with unique pairs of IDs (from the fourth column)\n",
        "    and labels (from the third column).\n",
        "\n",
        "    Parameters:\n",
        "    csv_file_path (str): The path to the CSV file.\n",
        "\n",
        "    Returns:\n",
        "    dict: A dictionary with IDs as keys and labels as values.\n",
        "    \"\"\"\n",
        "\n",
        "    # Dictionary to store the unique pairs\n",
        "    classes = {}\n",
        "\n",
        "    # Read the CSV file\n",
        "    with open(csv_file_path, mode='r', newline='', encoding='utf-8') as file:\n",
        "        csv_reader = csv.reader(file)\n",
        "\n",
        "        # Skip the CSV header\n",
        "        next(csv_reader)\n",
        "\n",
        "        for row in csv_reader:\n",
        "            label = row[2]       # Third column\n",
        "            id = int(row[3])     # Fourth column\n",
        "\n",
        "            # Add the pair to the dictionary if it doesn't already exist\n",
        "            if id not in classes:\n",
        "                classes[id] = label\n",
        "\n",
        "    return classes\n",
        "\n",
        "def show_grid(dataset: GroceryStoreDataset, classes: dict) -> None:\n",
        "    \"\"\"Shows a grid with random images taken from the dataset.\n",
        "\n",
        "    Args:\n",
        "        dataset: the dataset containing the images.\n",
        "        process: a function to apply on the images before showing them.\n",
        "    \"\"\"\n",
        "    fig = plt.figure(figsize=(15, 5))\n",
        "    indices_random = np.random.randint(10, size=10, high=len(classes.keys()))\n",
        "\n",
        "    for count, idx in enumerate(indices_random):\n",
        "        fig.add_subplot(2, 5, count + 1)\n",
        "        item = dataset.__getitem__(idx) # (Tensor, idx)\n",
        "        title = classes[item[1]]\n",
        "        plt.title(title)\n",
        "        image_processed = item[0]\n",
        "        plt.imshow(T.ToPILImage()(image_processed))\n",
        "        plt.axis(\"off\")\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "def parse_compose(v):\n",
        "  res = []\n",
        "  for t in str(v).split(\"\\n\")[1:]:\n",
        "    res.append(t.strip(\"    \"))\n",
        "  return res[:-1]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2F1cwsSnH8qO"
      },
      "source": [
        "### Configuration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V5mITuuvH8qP",
        "outputId": "259012bd-e69e-4ea7-c539-01efa313323e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "All good, a GPU is available\n"
          ]
        }
      ],
      "source": [
        "fix_random(seed=42)\n",
        "\n",
        "device = \"cpu\"\n",
        "if torch.cuda.is_available():\n",
        "  print(\"All good, a GPU is available\")\n",
        "  device = torch.device(\"cuda:0\")\n",
        "else:\n",
        "  print(\"Please set GPU via Edit -> Notebook Settings\")\n",
        "\n",
        "cfg = {\n",
        "    \"resize_size\": 256,\n",
        "    \"crop_size\": 224,\n",
        "    \"batch_size\": 32,\n",
        "    \"num_epochs\": 50,\n",
        "    \"lr\": 1e-3,\n",
        "    \"wd\": 5e-4,\n",
        "    \"step_size\": 5\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a1e0PtlZH8qQ"
      },
      "source": [
        "### Data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [],
      "source": [
        "# In  order to convert integer classes into their literal\n",
        "classes = extract_classes(csv_file_path = 'GroceryStoreDataset/dataset/classes.csv')\n",
        "\n",
        "cfg[\"augment\"] = True   # True/False\n",
        "data_transforms = {\n",
        "    \"train\": T.Compose([\n",
        "                        T.Resize((cfg[\"crop_size\"],cfg[\"crop_size\"])),\n",
        "                        T.ToTensor()\n",
        "                    ]),\n",
        "\n",
        "    \"val\": T.Compose([\n",
        "                        T.Resize((cfg[\"crop_size\"],cfg[\"crop_size\"])),\n",
        "                        T.ToTensor()\n",
        "                    ]),\n",
        "\n",
        "    \"test\": T.Compose([\n",
        "                        T.Resize((cfg[\"crop_size\"],cfg[\"crop_size\"])),\n",
        "                        T.ToTensor()\n",
        "                    ])\n",
        "}\n",
        "\n",
        "# Datasets\n",
        "data_train = GroceryStoreDataset(split=\"train\", transform=data_transforms[\"train\"] if cfg[\"augment\"] else None)\n",
        "data_val = GroceryStoreDataset(split=\"val\", transform=data_transforms[\"val\"])\n",
        "data_test = GroceryStoreDataset(split=\"test\", transform=data_transforms[\"test\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Augmented dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [],
      "source": [
        "cfg[\"augment\"] = True   # True/False\n",
        "data_transforms = {\n",
        "    \"train\": T.Compose([\n",
        "                        T.RandomResizedCrop(cfg[\"crop_size\"]),\n",
        "                        T.RandomHorizontalFlip(),\n",
        "                        T.RandomVerticalFlip(),\n",
        "                        T.RandomRotation(180),                              # rotation in all directions\n",
        "                        T.RandomAffine(degrees=0, translate=(0.1, 0.1)),    # translations of 10%\n",
        "                        T.RandomPerspective(distortion_scale=0.1, p=0.5),\n",
        "                        T.ToTensor(),\n",
        "                    ]),\n",
        "\n",
        "    \"val\": T.Compose([\n",
        "                        T.Resize(cfg[\"resize_size\"]),\n",
        "                        T.CenterCrop(cfg[\"crop_size\"]),\n",
        "                        T.ToTensor(),\n",
        "                    ]),\n",
        "\n",
        "    \"test\": T.Compose([\n",
        "                        T.Resize(cfg[\"resize_size\"]),\n",
        "                        T.ToTensor()\n",
        "                    ])\n",
        "}\n",
        "\n",
        "# Datasets\n",
        "data_train_augm = GroceryStoreDataset(split=\"train\", transform=data_transforms[\"train\"] if cfg[\"augment\"] else None)\n",
        "data_val_augm = GroceryStoreDataset(split=\"val\", transform=data_transforms[\"val\"])\n",
        "data_test_augm = GroceryStoreDataset(split=\"test\", transform=data_transforms[\"test\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Trainer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [],
      "source": [
        "# For automating batching\n",
        "\n",
        "loader_train = DataLoader(\n",
        "    data_train,\n",
        "    batch_size=cfg[\"batch_size\"],\n",
        "    shuffle=True,\n",
        "    pin_memory=True\n",
        ")\n",
        "loader_val = DataLoader(\n",
        "    data_val,\n",
        "    batch_size=cfg[\"batch_size\"],\n",
        "    shuffle=False\n",
        ")\n",
        "loader_test = DataLoader(\n",
        "    data_test,\n",
        "    batch_size=cfg[\"batch_size\"],\n",
        "    shuffle=False\n",
        ")\n",
        "\n",
        "loader_train_augm = DataLoader(\n",
        "    data_train_augm,\n",
        "    batch_size=cfg[\"batch_size\"],\n",
        "    shuffle=True,\n",
        "    pin_memory=True\n",
        ")\n",
        "loader_val_augm = DataLoader(\n",
        "    data_val_augm,\n",
        "    batch_size=cfg[\"batch_size\"],\n",
        "    shuffle=False\n",
        ")\n",
        "loader_test_augm = DataLoader(\n",
        "    data_test_augm,\n",
        "    batch_size=cfg[\"batch_size\"],\n",
        "    shuffle=False\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'nn' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[1], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mTrainer\u001b[39;00m:\n\u001b[0;32m      2\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m      3\u001b[0m             model: nn\u001b[38;5;241m.\u001b[39mModule,\n\u001b[0;32m      4\u001b[0m             train_loader: DataLoader,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m      8\u001b[0m             num_classes: \u001b[38;5;28mint\u001b[39m\n\u001b[0;32m      9\u001b[0m         ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m     10\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrain_loader \u001b[38;5;241m=\u001b[39m train_loader\n",
            "Cell \u001b[1;32mIn[1], line 3\u001b[0m, in \u001b[0;36mTrainer\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mTrainer\u001b[39;00m:\n\u001b[0;32m      2\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m----> 3\u001b[0m             model: \u001b[43mnn\u001b[49m\u001b[38;5;241m.\u001b[39mModule,\n\u001b[0;32m      4\u001b[0m             train_loader: DataLoader,\n\u001b[0;32m      5\u001b[0m             val_loader: DataLoader,\n\u001b[0;32m      6\u001b[0m             test_loader: DataLoader,\n\u001b[0;32m      7\u001b[0m             device: torch\u001b[38;5;241m.\u001b[39mdevice,\n\u001b[0;32m      8\u001b[0m             num_classes: \u001b[38;5;28mint\u001b[39m\n\u001b[0;32m      9\u001b[0m         ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m     10\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrain_loader \u001b[38;5;241m=\u001b[39m train_loader\n\u001b[0;32m     11\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mval_loader \u001b[38;5;241m=\u001b[39m val_loader\n",
            "\u001b[1;31mNameError\u001b[0m: name 'nn' is not defined"
          ]
        }
      ],
      "source": [
        "class Trainer:\n",
        "    def __init__(self,\n",
        "            model: nn.Module,\n",
        "            train_loader: DataLoader,\n",
        "            val_loader: DataLoader,\n",
        "            test_loader: DataLoader,\n",
        "            device: torch.device,\n",
        "            num_classes: int\n",
        "        ) -> None:\n",
        "        self.train_loader = train_loader\n",
        "        self.val_loader = val_loader\n",
        "        self.test_loader = test_loader\n",
        "        self.device = device\n",
        "        self.num_classes = num_classes\n",
        "        self.num_epochs = cfg[\"num_epochs\"]\n",
        "\n",
        "        self.model = model.to(device)\n",
        "        self.optimizer = AdamW(self.model.parameters(), lr=cfg[\"lr\"], weight_decay=cfg[\"wd\"])\n",
        "        num_steps = self.num_epochs * len(train_loader)\n",
        "        if cfg[\"scheduled\"]:\n",
        "            self.scheduler = OneCycleLR(self.optimizer, cfg[\"lr\"], total_steps=num_steps)\n",
        "        else:\n",
        "            self.scheduler = LambdaLR(self.optimizer, lr_lambda=lambda epoch: 1.0)\n",
        "\n",
        "        self.step = 0\n",
        "        self.best_acc = 0.0\n",
        "\n",
        "        wandb.init(name=cfg[\"run_name\"], entity=WANDB_USER, project=WANDB_PROJECT, config=cfg)\n",
        "        self.ckpt_path = Path(\"ckpts\")\n",
        "        self.ckpt_path.mkdir(exist_ok=True)\n",
        "\n",
        "    def logfn(self, values: Dict[str, Any]) -> None:\n",
        "        wandb.log(values, step=self.step, commit=False)\n",
        "\n",
        "    def train(self) -> None:\n",
        "        self.training_time = time.time()\n",
        "        for _ in tqdm(range(self.num_epochs), desc=\"Epoch\"):\n",
        "            self.model.train()\n",
        "\n",
        "            for imgs, labels in self.train_loader:\n",
        "                imgs = imgs.to(self.device)\n",
        "                labels = labels.to(self.device)\n",
        "\n",
        "                pred = self.model(imgs)\n",
        "                # print(pred.shape, labels.shape)\n",
        "                loss = F.cross_entropy(pred, labels)\n",
        "\n",
        "                self.optimizer.zero_grad()\n",
        "                loss.backward()\n",
        "                self.optimizer.step()\n",
        "                self.scheduler.step()\n",
        "\n",
        "                if self.step % 10 == 0:\n",
        "                    self.logfn({\"train/loss\": loss.item()})\n",
        "                    self.logfn({\"train/lr\": self.scheduler.get_last_lr()[0]})\n",
        "\n",
        "                self.step += 1\n",
        "\n",
        "            self.eval(\"train\")\n",
        "            self.eval(\"val\")\n",
        "\n",
        "        wandb.finish()\n",
        "        self.training_time = time.time() - self.training_time\n",
        "\n",
        "\n",
        "    # def test(self) -> None:\n",
        "    #     wandb.init(name=cfg[\"run_name\"]+\"_test\", entity=WANDB_USER, project=WANDB_PROJECT, config=cfg)\n",
        "    #     self.eval(\"test\")\n",
        "    #     wandb.finish()\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def eval(self, split: str) -> None:\n",
        "        self.model.eval()\n",
        "\n",
        "        if split == \"train\":\n",
        "            loader = self.train_loader\n",
        "        elif split == \"val\":\n",
        "            loader = self.val_loader\n",
        "        # elif split == \"test\":\n",
        "        #     loader = self.test_loader\n",
        "        else:\n",
        "            raise ValueError(f\"Unknown split: {split}\")\n",
        "\n",
        "        acc = Accuracy(\"multiclass\", num_classes=self.num_classes).to(self.device)\n",
        "\n",
        "        losses = []\n",
        "        for imgs, labels in loader:\n",
        "            imgs = imgs.to(self.device)\n",
        "            labels = labels.to(self.device)\n",
        "\n",
        "            pred = self.model(imgs)\n",
        "            loss = F.cross_entropy(pred, labels)\n",
        "            losses.append(loss.item())\n",
        "\n",
        "            pred_softmax = F.softmax(pred, dim=-1)\n",
        "            acc(pred_softmax, labels)\n",
        "\n",
        "        loss = sum(losses) / len(losses)\n",
        "        accuracy = acc.compute()\n",
        "\n",
        "        self.logfn({f\"{split}/loss\": loss})\n",
        "        self.logfn({f\"{split}/acc\": accuracy})\n",
        "\n",
        "        if accuracy > self.best_acc and split == \"val\":\n",
        "            self.best_acc = accuracy\n",
        "            torch.save(self.model.state_dict(), self.ckpt_path / f\"{wandb.run.name}.pt\")\n",
        "            self.best_model = copy.deepcopy(self.model)\n",
        "\n",
        "    def save_model_params(self, cfg, data_transforms):\n",
        "        model_dir = os.path.join(\"params\", f\"{cfg['model']}_model\")\n",
        "        os.makedirs(model_dir, exist_ok=True)\n",
        "\n",
        "        timestamp = datetime.now().strftime(\"%Y-%m-%d-%H-%M-%S\")  # in order not to have duplicates\n",
        "        training_time_formatted = time.strftime('%H-%M-%S', time.gmtime(self.training_time))\n",
        "        file_name = f\"ACC={self.best_acc:.2f}____TT={training_time_formatted}____TM={timestamp}.json\"\n",
        "        file_path = os.path.join(model_dir, file_name)\n",
        "\n",
        "        combined_params = {\n",
        "            \"cfg\": cfg,\n",
        "            \"data_transforms\": {k: parse_compose(v) for k,v in data_transforms.items()},\n",
        "            \"model_structure\" : parse_compose(self.model)\n",
        "        }\n",
        "\n",
        "        with open(file_path, 'w') as f:\n",
        "            json.dump(combined_params, f, indent=4)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# !wandb login --relogin 301c5db1df292fea814603255510519eb6564aff"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1de1sRClH8qS"
      },
      "source": [
        "### Models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### V1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VIQb1a3C-SuF"
      },
      "outputs": [],
      "source": [
        "class ProjectCNN_v1(nn.Module):\n",
        "    def __init__(self, n_classes, input_dim):\n",
        "        super(ProjectCNN_v1, self).__init__()\n",
        "        self.input_dim=input_dim\n",
        "\n",
        "        ## Convolutional layers\n",
        "        K,S,P=3,2,0\n",
        "        self.conv1 = nn.Conv2d(in_channels=3, out_channels=32, kernel_size=7, stride=S, padding=P)  # 32 filters for simpler details\n",
        "        self.conv2 = nn.Conv2d(in_channels=32, out_channels=64, kernel_size=5, stride=S, padding=P) # 64 filters for more complex details\n",
        "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2, padding=0)    # reduces spatial dimensions, decreases computational load, improves  \n",
        "                                                                        # translation invariance, highlights prominent features, and helps \n",
        "                                                                        # prevent overfitting.\n",
        "        \n",
        "        self._to_linear = None\n",
        "        self._set_conv_output_size()\n",
        "        self.fc1 = nn.Linear(self._to_linear, 128)\n",
        "        self.fc2 = nn.Linear(128, n_classes)\n",
        "\n",
        "    def _set_conv_output_size(self):\n",
        "        x = torch.randn(1, 3, *self.input_dim)\n",
        "        x=self._forward(x)\n",
        "        self._to_linear = x.numel()\n",
        "\n",
        "    def _forward(self,x):\n",
        "        x = self.pool(F.relu(self.conv1(x)))  # 32 x 112 x 112\n",
        "        x = self.pool(F.relu(self.conv2(x)))  # 64 x 56 x 56\n",
        "        return x         \n",
        "\n",
        "    def forward(self, x):\n",
        "        x=self._forward(x)\n",
        "        x = x.view(x.size(0), -1) # to feed the FCs       \n",
        "        x = F.relu(self.fc1(x))               \n",
        "        x = self.fc2(x)                       \n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "----------------------------------------------------------------\n",
            "        Layer (type)               Output Shape         Param #\n",
            "================================================================\n",
            "            Conv2d-1         [-1, 32, 109, 109]           4,736\n",
            "         MaxPool2d-2           [-1, 32, 54, 54]               0\n",
            "            Conv2d-3           [-1, 64, 25, 25]          51,264\n",
            "         MaxPool2d-4           [-1, 64, 12, 12]               0\n",
            "            Linear-5                  [-1, 128]       1,179,776\n",
            "            Linear-6                   [-1, 43]           5,547\n",
            "================================================================\n",
            "Total params: 1,241,323\n",
            "Trainable params: 1,241,323\n",
            "Non-trainable params: 0\n",
            "----------------------------------------------------------------\n",
            "Input size (MB): 0.57\n",
            "Forward/backward pass size (MB): 3.99\n",
            "Params size (MB): 4.74\n",
            "Estimated Total Size (MB): 9.30\n",
            "----------------------------------------------------------------\n"
          ]
        }
      ],
      "source": [
        "model = ProjectCNN_v1(n_classes=len(classes),input_dim=(cfg[\"crop_size\"], cfg[\"crop_size\"]))\n",
        "if next(model.parameters()).device != device:\n",
        "    model.to(device)\n",
        "summary(\n",
        "    model,\n",
        "    input_size=(3, cfg[\"crop_size\"], cfg[\"crop_size\"])\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "Finishing last run (ID:h19iraln) before initializing another..."
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "80bf56f08358423c89cf008e600da32c",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "VBox(children=(Label(value='0.005 MB of 0.007 MB uploaded\\r'), FloatProgress(value=0.832626188734455, max=1.0)…"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<style>\n",
              "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
              "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
              "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
              "    </style>\n",
              "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>train/acc</td><td>▁▃▅▆▇▇▇█████</td></tr><tr><td>train/loss</td><td>█▇█▆▆▅▄▄▃▃▃▃▃▂▂▂▂▁▂▁▁▂▂▁▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train/lr</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val/acc</td><td>▁▁▅▅▅▄█▅▁▆▄▆</td></tr><tr><td>val/loss</td><td>▂▂▁▁▂▄▄▅▆▇▇█</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>train/acc</td><td>0.99811</td></tr><tr><td>train/loss</td><td>0.00415</td></tr><tr><td>train/lr</td><td>0.001</td></tr><tr><td>val/acc</td><td>0.28041</td></tr><tr><td>val/loss</td><td>5.23389</td></tr></table><br/></div></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">v1_big_conv</strong> at: <a href='https://wandb.ai/davidecrociati/assignments%20part%20two/runs/h19iraln' target=\"_blank\">https://wandb.ai/davidecrociati/assignments%20part%20two/runs/h19iraln</a><br/> View project at: <a href='https://wandb.ai/davidecrociati/assignments%20part%20two' target=\"_blank\">https://wandb.ai/davidecrociati/assignments%20part%20two</a><br/>Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Find logs at: <code>.\\wandb\\run-20240807_120147-h19iraln\\logs</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information."
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Successfully finished last run (ID:h19iraln). Initializing new run:<br/>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "b84ceb75618f495591eec8c234cc2643",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.011111111111111112, max=1.0…"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.17.5"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>c:\\Users\\Davide\\Documents\\GitHub\\IPCV-assignments\\part_two\\pelle\\wandb\\run-20240807_120553-ezrrvsh1</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/davidecrociati/assignments%20part%20two/runs/ezrrvsh1' target=\"_blank\">v1_big_conv_augm</a></strong> to <a href='https://wandb.ai/davidecrociati/assignments%20part%20two' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/davidecrociati/assignments%20part%20two' target=\"_blank\">https://wandb.ai/davidecrociati/assignments%20part%20two</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/davidecrociati/assignments%20part%20two/runs/ezrrvsh1' target=\"_blank\">https://wandb.ai/davidecrociati/assignments%20part%20two/runs/ezrrvsh1</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "ebe1939bccfc46459bfca5a8edbf3819",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Epoch:   0%|          | 0/50 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[95], line 15\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m# Wandb key: 3f0834114b4b33656e70323616fa377c30c83542\u001b[39;00m\n\u001b[0;32m      7\u001b[0m trainer \u001b[38;5;241m=\u001b[39m Trainer(\n\u001b[0;32m      8\u001b[0m     model,\n\u001b[0;32m      9\u001b[0m     loader_train_augm,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     13\u001b[0m     num_classes\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mlen\u001b[39m(classes\u001b[38;5;241m.\u001b[39mkeys())\n\u001b[0;32m     14\u001b[0m )\n\u001b[1;32m---> 15\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBest val acc = \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrainer\u001b[38;5;241m.\u001b[39mbest_acc\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.3f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     17\u001b[0m \u001b[38;5;66;03m# Log results\u001b[39;00m\n",
            "Cell \u001b[1;32mIn[12], line 59\u001b[0m, in \u001b[0;36mTrainer.train\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     55\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlogfn({\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain/lr\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscheduler\u001b[38;5;241m.\u001b[39mget_last_lr()[\u001b[38;5;241m0\u001b[39m]})\n\u001b[0;32m     57\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstep \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m---> 59\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meval\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtrain\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     60\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39meval(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mval\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     62\u001b[0m wandb\u001b[38;5;241m.\u001b[39mfinish()\n",
            "File \u001b[1;32mc:\\Users\\Davide\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\utils\\_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    112\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[0;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m    114\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[1;32m--> 115\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
            "Cell \u001b[1;32mIn[12], line 87\u001b[0m, in \u001b[0;36mTrainer.eval\u001b[1;34m(self, split)\u001b[0m\n\u001b[0;32m     84\u001b[0m acc \u001b[38;5;241m=\u001b[39m Accuracy(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmulticlass\u001b[39m\u001b[38;5;124m\"\u001b[39m, num_classes\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_classes)\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[0;32m     86\u001b[0m losses \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m---> 87\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m imgs, labels \u001b[38;5;129;01min\u001b[39;00m loader:\n\u001b[0;32m     88\u001b[0m     imgs \u001b[38;5;241m=\u001b[39m imgs\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[0;32m     89\u001b[0m     labels \u001b[38;5;241m=\u001b[39m labels\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice)\n",
            "File \u001b[1;32mc:\\Users\\Davide\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:631\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    628\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    629\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    630\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 631\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    632\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    633\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    634\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    635\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
            "File \u001b[1;32mc:\\Users\\Davide\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:675\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    673\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    674\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m--> 675\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    676\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[0;32m    677\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
            "File \u001b[1;32mc:\\Users\\Davide\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:51\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     49\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[0;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 51\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
            "File \u001b[1;32mc:\\Users\\Davide\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:51\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     49\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[0;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 51\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
            "Cell \u001b[1;32mIn[3], line 20\u001b[0m, in \u001b[0;36mGroceryStoreDataset.__getitem__\u001b[1;34m(self, idx)\u001b[0m\n\u001b[0;32m     17\u001b[0m label \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlabels[idx]\n\u001b[0;32m     19\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransform:\n\u001b[1;32m---> 20\u001b[0m     img \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     22\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m img, label\n",
            "File \u001b[1;32mc:\\Users\\Davide\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torchvision\\transforms\\transforms.py:95\u001b[0m, in \u001b[0;36mCompose.__call__\u001b[1;34m(self, img)\u001b[0m\n\u001b[0;32m     93\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, img):\n\u001b[0;32m     94\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransforms:\n\u001b[1;32m---> 95\u001b[0m         img \u001b[38;5;241m=\u001b[39m \u001b[43mt\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     96\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m img\n",
            "File \u001b[1;32mc:\\Users\\Davide\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
            "File \u001b[1;32mc:\\Users\\Davide\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
            "File \u001b[1;32mc:\\Users\\Davide\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torchvision\\transforms\\transforms.py:973\u001b[0m, in \u001b[0;36mRandomResizedCrop.forward\u001b[1;34m(self, img)\u001b[0m\n\u001b[0;32m    965\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    966\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[0;32m    967\u001b[0m \u001b[38;5;124;03m    img (PIL Image or Tensor): Image to be cropped and resized.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    970\u001b[0m \u001b[38;5;124;03m    PIL Image or Tensor: Randomly cropped and resized image.\u001b[39;00m\n\u001b[0;32m    971\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    972\u001b[0m i, j, h, w \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_params(img, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscale, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mratio)\n\u001b[1;32m--> 973\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresized_crop\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mh\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mw\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msize\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minterpolation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mantialias\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mantialias\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32mc:\\Users\\Davide\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torchvision\\transforms\\functional.py:641\u001b[0m, in \u001b[0;36mresized_crop\u001b[1;34m(img, top, left, height, width, size, interpolation, antialias)\u001b[0m\n\u001b[0;32m    639\u001b[0m     _log_api_usage_once(resized_crop)\n\u001b[0;32m    640\u001b[0m img \u001b[38;5;241m=\u001b[39m crop(img, top, left, height, width)\n\u001b[1;32m--> 641\u001b[0m img \u001b[38;5;241m=\u001b[39m \u001b[43mresize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msize\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minterpolation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mantialias\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mantialias\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    642\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m img\n",
            "File \u001b[1;32mc:\\Users\\Davide\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torchvision\\transforms\\functional.py:468\u001b[0m, in \u001b[0;36mresize\u001b[1;34m(img, size, interpolation, max_size, antialias)\u001b[0m\n\u001b[0;32m    466\u001b[0m         warnings\u001b[38;5;241m.\u001b[39mwarn(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAnti-alias option is always applied for PIL Image input. Argument antialias is ignored.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    467\u001b[0m     pil_interpolation \u001b[38;5;241m=\u001b[39m pil_modes_mapping[interpolation]\n\u001b[1;32m--> 468\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF_pil\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minterpolation\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpil_interpolation\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    470\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m F_t\u001b[38;5;241m.\u001b[39mresize(img, size\u001b[38;5;241m=\u001b[39moutput_size, interpolation\u001b[38;5;241m=\u001b[39minterpolation\u001b[38;5;241m.\u001b[39mvalue, antialias\u001b[38;5;241m=\u001b[39mantialias)\n",
            "File \u001b[1;32mc:\\Users\\Davide\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torchvision\\transforms\\_functional_pil.py:250\u001b[0m, in \u001b[0;36mresize\u001b[1;34m(img, size, interpolation)\u001b[0m\n\u001b[0;32m    247\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28misinstance\u001b[39m(size, \u001b[38;5;28mlist\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(size) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m2\u001b[39m):\n\u001b[0;32m    248\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGot inappropriate size arg: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msize\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 250\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mimg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresize\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mtuple\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43msize\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m:\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minterpolation\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32mc:\\Users\\Davide\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\PIL\\Image.py:2222\u001b[0m, in \u001b[0;36mImage.resize\u001b[1;34m(self, size, resample, box, reducing_gap)\u001b[0m\n\u001b[0;32m   2210\u001b[0m         \u001b[38;5;28mself\u001b[39m \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m   2211\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreduce(factor, box\u001b[38;5;241m=\u001b[39mreduce_box)\n\u001b[0;32m   2212\u001b[0m             \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mcallable\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreduce)\n\u001b[0;32m   2213\u001b[0m             \u001b[38;5;28;01melse\u001b[39;00m Image\u001b[38;5;241m.\u001b[39mreduce(\u001b[38;5;28mself\u001b[39m, factor, box\u001b[38;5;241m=\u001b[39mreduce_box)\n\u001b[0;32m   2214\u001b[0m         )\n\u001b[0;32m   2215\u001b[0m         box \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m   2216\u001b[0m             (box[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m-\u001b[39m reduce_box[\u001b[38;5;241m0\u001b[39m]) \u001b[38;5;241m/\u001b[39m factor_x,\n\u001b[0;32m   2217\u001b[0m             (box[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m-\u001b[39m reduce_box[\u001b[38;5;241m1\u001b[39m]) \u001b[38;5;241m/\u001b[39m factor_y,\n\u001b[0;32m   2218\u001b[0m             (box[\u001b[38;5;241m2\u001b[39m] \u001b[38;5;241m-\u001b[39m reduce_box[\u001b[38;5;241m0\u001b[39m]) \u001b[38;5;241m/\u001b[39m factor_x,\n\u001b[0;32m   2219\u001b[0m             (box[\u001b[38;5;241m3\u001b[39m] \u001b[38;5;241m-\u001b[39m reduce_box[\u001b[38;5;241m1\u001b[39m]) \u001b[38;5;241m/\u001b[39m factor_y,\n\u001b[0;32m   2220\u001b[0m         )\n\u001b[1;32m-> 2222\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_new(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mim\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresize\u001b[49m\u001b[43m(\u001b[49m\u001b[43msize\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresample\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbox\u001b[49m\u001b[43m)\u001b[49m)\n",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
          ]
        },
        {
          "ename": "",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31mSi è verificato un arresto anomalo del Kernel durante l'esecuzione del codice nella cella attiva o in una cella precedente. \n",
            "\u001b[1;31mEsaminare il codice nelle celle per identificare una possibile causa dell'errore. \n",
            "\u001b[1;31mPer altre informazioni, fare clic<a href='https://aka.ms/vscodeJupyterKernelCrash'>qui</a>. \n",
            "\u001b[1;31mPer ulteriori dettagli, visualizzare Jupyter <a href='command:jupyter.viewOutput'>log</a>."
          ]
        }
      ],
      "source": [
        "cfg[\"model\"] = \"v1_big_conv_augm\"\n",
        "cfg[\"scheduled\"]=False\n",
        "cfg[\"run_name\"] = f\"{cfg['model']}\" + (\"_scheduled\" if cfg[\"scheduled\"] else \"\")\n",
        "\n",
        "# Wandb key: 3f0834114b4b33656e70323616fa377c30c83542\n",
        "\n",
        "trainer = Trainer(\n",
        "    model,\n",
        "    loader_train_augm,\n",
        "    loader_val_augm,\n",
        "    loader_test_augm,\n",
        "    device,\n",
        "    num_classes=len(classes.keys())\n",
        ")\n",
        "trainer.train()\n",
        "print(f\"Best val acc = {trainer.best_acc:.3f}\")\n",
        "# Log results\n",
        "trainer.save_model_params(cfg, data_transforms)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### V2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y92sAoSf-SuG"
      },
      "outputs": [],
      "source": [
        "class ProjectCNN_v2(nn.Module):\n",
        "    def __init__(self, n_classes, input_dim):\n",
        "        super(ProjectCNN_v2, self).__init__()\n",
        "        self.input_dim=input_dim\n",
        "\n",
        "        K,S,P=3,2,1\n",
        "\n",
        "        # Convolutional layers\n",
        "        self.conv1 = nn.Conv2d(in_channels=3, out_channels=32, kernel_size=K, stride=S, padding=P)\n",
        "        self.conv2 = nn.Conv2d(in_channels=32, out_channels=64, kernel_size=K, stride=S, padding=P)\n",
        "        self.conv3 = nn.Conv2d(in_channels=64, out_channels=128, kernel_size=K, stride=S, padding=P)\n",
        "        self.conv4 = nn.Conv2d(in_channels=128, out_channels=256, kernel_size=K, stride=S, padding=P)\n",
        "        self.conv5 = nn.Conv2d(in_channels=256, out_channels=512, kernel_size=K, stride=S, padding=P)\n",
        "\n",
        "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2, padding=0)\n",
        "        \n",
        "        self._to_linear = None\n",
        "        self._set_conv_output_size()\n",
        "        self.fc1 = nn.Linear(self._to_linear, 1024)\n",
        "        self.fc2 = nn.Linear(1024, 512)\n",
        "        self.fc3 = nn.Linear(512, n_classes)  \n",
        "\n",
        "    def _set_conv_output_size(self):\n",
        "        x = torch.randn(1, 3, *self.input_dim)\n",
        "        x=self._forward(x)\n",
        "        self._to_linear = x.numel()\n",
        " \n",
        "    def _forward(self,x):\n",
        "        x = self.pool(F.relu(self.conv1(x)))  # 32 x 112 x 112\n",
        "        x = self.pool(F.relu(self.conv2(x)))  # 64 x 56 x 56\n",
        "        x = self.pool(F.relu(self.conv3(x)))  # 128 x 28 x 28\n",
        "        x = self.pool(F.relu(self.conv4(x)))  # 256 x 14 x 14\n",
        "        x = self.pool(F.relu(self.conv5(x)))  # 512 x 7 x 7\n",
        "        return x\n",
        "\n",
        "    def forward(self, x):\n",
        "        x=self._forward(x)\n",
        "        x = x.view(x.size(0), -1)             \n",
        "        x = F.relu(self.fc1(x))               \n",
        "        x = F.relu(self.fc2(x))               \n",
        "        x = self.fc3(x)                       \n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "ename": "TypeError",
          "evalue": "conv2d() received an invalid combination of arguments - got (Tensor, Parameter, Parameter, tuple, tuple, tuple, int), but expected one of:\n * (Tensor input, Tensor weight, Tensor bias, tuple of ints stride, tuple of ints padding, tuple of ints dilation, int groups)\n      didn't match because some of the arguments have invalid types: (Tensor, !Parameter!, !Parameter!, !tuple of (float, float)!, !tuple of (int, int)!, !tuple of (int, int)!, int)\n * (Tensor input, Tensor weight, Tensor bias, tuple of ints stride, str padding, tuple of ints dilation, int groups)\n      didn't match because some of the arguments have invalid types: (Tensor, !Parameter!, !Parameter!, !tuple of (float, float)!, !tuple of (int, int)!, !tuple of (int, int)!, int)\n",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[63], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mProjectCNN_v2\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn_classes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mclasses\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43minput_dim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcfg\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcrop_size\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcfg\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcrop_size\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mnext\u001b[39m(model\u001b[38;5;241m.\u001b[39mparameters())\u001b[38;5;241m.\u001b[39mdevice \u001b[38;5;241m!=\u001b[39m device:\n\u001b[0;32m      3\u001b[0m     model\u001b[38;5;241m.\u001b[39mto(device)\n",
            "Cell \u001b[1;32mIn[62], line 18\u001b[0m, in \u001b[0;36mProjectCNN_v2.__init__\u001b[1;34m(self, n_classes, input_dim)\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpool \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mMaxPool2d(kernel_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m, stride\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m, padding\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m     17\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_to_linear \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m---> 18\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_conv_output_size\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     19\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfc1 \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mLinear(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_to_linear, \u001b[38;5;241m1024\u001b[39m)\n\u001b[0;32m     20\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfc2 \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mLinear(\u001b[38;5;241m1024\u001b[39m, \u001b[38;5;241m512\u001b[39m)\n",
            "Cell \u001b[1;32mIn[62], line 25\u001b[0m, in \u001b[0;36mProjectCNN_v2._get_conv_output_size\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_get_conv_output_size\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m     24\u001b[0m     x \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrandn(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minput_dim)\n\u001b[1;32m---> 25\u001b[0m     x\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     26\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_to_linear \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mnumel()\n",
            "Cell \u001b[1;32mIn[62], line 31\u001b[0m, in \u001b[0;36mProjectCNN_v2._forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     29\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpool(F\u001b[38;5;241m.\u001b[39mrelu(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconv1(x)))  \u001b[38;5;66;03m# 32 x 112 x 112\u001b[39;00m\n\u001b[0;32m     30\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpool(F\u001b[38;5;241m.\u001b[39mrelu(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconv2(x)))  \u001b[38;5;66;03m# 64 x 56 x 56\u001b[39;00m\n\u001b[1;32m---> 31\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpool(F\u001b[38;5;241m.\u001b[39mrelu(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv3\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m))  \u001b[38;5;66;03m# 128 x 28 x 28\u001b[39;00m\n\u001b[0;32m     32\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpool(F\u001b[38;5;241m.\u001b[39mrelu(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconv4(x)))  \u001b[38;5;66;03m# 256 x 14 x 14\u001b[39;00m\n\u001b[0;32m     33\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpool(F\u001b[38;5;241m.\u001b[39mrelu(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconv5(x)))  \u001b[38;5;66;03m# 512 x 7 x 7\u001b[39;00m\n",
            "File \u001b[1;32mc:\\Users\\Davide\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
            "File \u001b[1;32mc:\\Users\\Davide\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
            "File \u001b[1;32mc:\\Users\\Davide\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\conv.py:460\u001b[0m, in \u001b[0;36mConv2d.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    459\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 460\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_conv_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32mc:\\Users\\Davide\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\conv.py:456\u001b[0m, in \u001b[0;36mConv2d._conv_forward\u001b[1;34m(self, input, weight, bias)\u001b[0m\n\u001b[0;32m    452\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mzeros\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m    453\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mconv2d(F\u001b[38;5;241m.\u001b[39mpad(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode),\n\u001b[0;32m    454\u001b[0m                     weight, bias, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstride,\n\u001b[0;32m    455\u001b[0m                     _pair(\u001b[38;5;241m0\u001b[39m), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdilation, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgroups)\n\u001b[1;32m--> 456\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv2d\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    457\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdilation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgroups\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[1;31mTypeError\u001b[0m: conv2d() received an invalid combination of arguments - got (Tensor, Parameter, Parameter, tuple, tuple, tuple, int), but expected one of:\n * (Tensor input, Tensor weight, Tensor bias, tuple of ints stride, tuple of ints padding, tuple of ints dilation, int groups)\n      didn't match because some of the arguments have invalid types: (Tensor, !Parameter!, !Parameter!, !tuple of (float, float)!, !tuple of (int, int)!, !tuple of (int, int)!, int)\n * (Tensor input, Tensor weight, Tensor bias, tuple of ints stride, str padding, tuple of ints dilation, int groups)\n      didn't match because some of the arguments have invalid types: (Tensor, !Parameter!, !Parameter!, !tuple of (float, float)!, !tuple of (int, int)!, !tuple of (int, int)!, int)\n"
          ]
        }
      ],
      "source": [
        "model = ProjectCNN_v2(n_classes=len(classes),input_dim=(cfg[\"crop_size\"], cfg[\"crop_size\"]))\n",
        "if next(model.parameters()).device != device:\n",
        "    model.to(device)\n",
        "summary(\n",
        "    model,\n",
        "    input_size=(3, cfg[\"crop_size\"], cfg[\"crop_size\"])\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "----------------------------------------------------------------\n",
            "        Layer (type)               Output Shape         Param #\n",
            "================================================================\n",
            "            Conv2d-1         [-1, 32, 224, 224]             896\n",
            "         MaxPool2d-2         [-1, 32, 112, 112]               0\n",
            "            Conv2d-3         [-1, 64, 112, 112]          18,496\n",
            "         MaxPool2d-4           [-1, 64, 56, 56]               0\n",
            "            Linear-5                  [-1, 128]      25,690,240\n",
            "            Linear-6                   [-1, 43]           5,547\n",
            "================================================================\n",
            "Total params: 25,715,179\n",
            "Trainable params: 25,715,179\n",
            "Non-trainable params: 0\n",
            "----------------------------------------------------------------\n",
            "Input size (MB): 0.57\n",
            "Forward/backward pass size (MB): 22.97\n",
            "Params size (MB): 98.10\n",
            "Estimated Total Size (MB): 121.64\n",
            "----------------------------------------------------------------\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.17.5"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>c:\\Users\\Davide\\Documents\\GitHub\\IPCV-assignments\\part_two\\pelle\\wandb\\run-20240806_164141-xwf6bzi8</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/davidecrociati/assignments%20part%20two/runs/xwf6bzi8' target=\"_blank\">test: ProjectCNN_v2</a></strong> to <a href='https://wandb.ai/davidecrociati/assignments%20part%20two' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/davidecrociati/assignments%20part%20two' target=\"_blank\">https://wandb.ai/davidecrociati/assignments%20part%20two</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/davidecrociati/assignments%20part%20two/runs/xwf6bzi8' target=\"_blank\">https://wandb.ai/davidecrociati/assignments%20part%20two/runs/xwf6bzi8</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "2457c120733548dabb3e9705f80ef184",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Epoch:   0%|          | 0/50 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[80], line 22\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;66;03m# Wandb key: 3f0834114b4b33656e70323616fa377c30c83542\u001b[39;00m\n\u001b[0;32m     14\u001b[0m trainer \u001b[38;5;241m=\u001b[39m Trainer(\n\u001b[0;32m     15\u001b[0m     model,\n\u001b[0;32m     16\u001b[0m     loader_train,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     20\u001b[0m     num_classes\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mlen\u001b[39m(classes\u001b[38;5;241m.\u001b[39mkeys())\n\u001b[0;32m     21\u001b[0m )\n\u001b[1;32m---> 22\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     23\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBest val acc = \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrainer\u001b[38;5;241m.\u001b[39mbest_acc\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.3f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     24\u001b[0m \u001b[38;5;66;03m# Log results\u001b[39;00m\n",
            "Cell \u001b[1;32mIn[75], line 59\u001b[0m, in \u001b[0;36mTrainer.train\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     55\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlogfn({\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain/lr\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscheduler\u001b[38;5;241m.\u001b[39mget_last_lr()[\u001b[38;5;241m0\u001b[39m]})\n\u001b[0;32m     57\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstep \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m---> 59\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meval\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtrain\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     60\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39meval(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mval\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     62\u001b[0m wandb\u001b[38;5;241m.\u001b[39mfinish()\n",
            "File \u001b[1;32mc:\\Users\\Davide\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\utils\\_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    112\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[0;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m    114\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[1;32m--> 115\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
            "Cell \u001b[1;32mIn[75], line 93\u001b[0m, in \u001b[0;36mTrainer.eval\u001b[1;34m(self, split)\u001b[0m\n\u001b[0;32m     91\u001b[0m pred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel(imgs)\n\u001b[0;32m     92\u001b[0m loss \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mcross_entropy(pred, labels)\n\u001b[1;32m---> 93\u001b[0m losses\u001b[38;5;241m.\u001b[39mappend(\u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m     95\u001b[0m pred_softmax \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39msoftmax(pred, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m     96\u001b[0m acc(pred_softmax, labels)\n",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "cfg[\"model\"] = \"v2_augm\"\n",
        "model = ProjectCNN_v2(n_classes=len(classes),input_dim=(cfg[\"crop_size\"], cfg[\"crop_size\"]))\n",
        "if next(model.parameters()).device != device:\n",
        "    model.to(device)\n",
        "summary(\n",
        "    model,\n",
        "    input_size=(3, cfg[\"crop_size\"], cfg[\"crop_size\"])\n",
        ")\n",
        "cfg[\"scheduled\"]=False\n",
        "cfg[\"run_name\"] = f\"{cfg['model']}\" + (\"_scheduled\" if cfg[\"scheduled\"] else \"\")\n",
        "\n",
        "# Wandb key: 3f0834114b4b33656e70323616fa377c30c83542\n",
        "\n",
        "trainer = Trainer(\n",
        "    model,\n",
        "    loader_train_augm,\n",
        "    loader_val_augm,\n",
        "    loader_test_augm,\n",
        "    device,\n",
        "    num_classes=len(classes.keys())\n",
        ")\n",
        "trainer.train()\n",
        "print(f\"Best val acc = {trainer.best_acc:.3f}\")\n",
        "# Log results\n",
        "trainer.save_model_params(cfg, data_transforms)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "----------------------------------------------------------------\n",
            "        Layer (type)               Output Shape         Param #\n",
            "================================================================\n",
            "            Conv2d-1         [-1, 32, 224, 224]             896\n",
            "         MaxPool2d-2         [-1, 32, 112, 112]               0\n",
            "            Conv2d-3         [-1, 64, 112, 112]          18,496\n",
            "         MaxPool2d-4           [-1, 64, 56, 56]               0\n",
            "            Linear-5                  [-1, 128]      25,690,240\n",
            "            Linear-6                   [-1, 43]           5,547\n",
            "================================================================\n",
            "Total params: 25,715,179\n",
            "Trainable params: 25,715,179\n",
            "Non-trainable params: 0\n",
            "----------------------------------------------------------------\n",
            "Input size (MB): 0.57\n",
            "Forward/backward pass size (MB): 22.97\n",
            "Params size (MB): 98.10\n",
            "Estimated Total Size (MB): 121.64\n",
            "----------------------------------------------------------------\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "Finishing last run (ID:xwf6bzi8) before initializing another..."
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "dca8486cf5a84935b37246d0bbf570a6",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<style>\n",
              "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
              "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
              "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
              "    </style>\n",
              "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>train/acc</td><td>▁▅▇█████████████████████████████████████</td></tr><tr><td>train/loss</td><td>█▄▃▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train/lr</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val/acc</td><td>▃█▅▁▁▃▇█▇▅▅▄▄▃▅▄▃▄▅▅▃▄▄▃▃▃▄▃▃▃▃▃▃▃▂▂▃▁▂▂</td></tr><tr><td>val/loss</td><td>▁▁▂▃▄▃▄▄▄▄▅▅▅▆▆▆▆▆▆▆▆▇▇▇▇▇▇▇▇▇▇▇▇▇██████</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>train/acc</td><td>1.0</td></tr><tr><td>train/loss</td><td>0.0</td></tr><tr><td>train/lr</td><td>0.001</td></tr><tr><td>val/acc</td><td>0.20946</td></tr><tr><td>val/loss</td><td>7.76225</td></tr></table><br/></div></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">test: ProjectCNN_v2</strong> at: <a href='https://wandb.ai/davidecrociati/assignments%20part%20two/runs/xwf6bzi8' target=\"_blank\">https://wandb.ai/davidecrociati/assignments%20part%20two/runs/xwf6bzi8</a><br/> View project at: <a href='https://wandb.ai/davidecrociati/assignments%20part%20two' target=\"_blank\">https://wandb.ai/davidecrociati/assignments%20part%20two</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Find logs at: <code>.\\wandb\\run-20240806_164141-xwf6bzi8\\logs</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information."
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Successfully finished last run (ID:xwf6bzi8). Initializing new run:<br/>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "3e9f9e72ae2441d791903f7391f21733",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.01128888888957186, max=1.0)…"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.17.5"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>c:\\Users\\Davide\\Documents\\GitHub\\IPCV-assignments\\part_two\\pelle\\wandb\\run-20240806_165833-bou9vr94</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/davidecrociati/assignments%20part%20two/runs/bou9vr94' target=\"_blank\">test: ProjectCNN_v22</a></strong> to <a href='https://wandb.ai/davidecrociati/assignments%20part%20two' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/davidecrociati/assignments%20part%20two' target=\"_blank\">https://wandb.ai/davidecrociati/assignments%20part%20two</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/davidecrociati/assignments%20part%20two/runs/bou9vr94' target=\"_blank\">https://wandb.ai/davidecrociati/assignments%20part%20two/runs/bou9vr94</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "d5065d33451c4784a9c8034f4236045a",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Epoch:   0%|          | 0/50 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "ed6bc7e5a584488aa2818e7469d0da98",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "VBox(children=(Label(value='0.001 MB of 0.007 MB uploaded\\r'), FloatProgress(value=0.2108626198083067, max=1.0…"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<style>\n",
              "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
              "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
              "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
              "    </style>\n",
              "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>train/acc</td><td>▁▂▂▃▄▄▅▄▅▅▆▆▅▆▆▆▅▆▆▆▆▆▇▇▇▇▇▆▆▇▇▇▇▇▇▇▇█▇█</td></tr><tr><td>train/loss</td><td>█▆▆▃▃▃▆▅▄▄▃▂▅▂▃▃▃▆▃▂▃▅▃▃▃▄▃▅▃▂▃▄▄▄▂▂▂▂▁▃</td></tr><tr><td>train/lr</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val/acc</td><td>▁▂▄▃▅▅▅▇▆▅▆▆▅▄▆▆▂▅▇▅▄▆▆▇▇▇▄▅▄▄▇▆▆▅▆▆▆███</td></tr><tr><td>val/loss</td><td>▆▅▂▃▁▃▂▂▁▄▁▃▄▃▁▂▄▄▂▂▄▄▄▃▃▂▄▄█▅▃▃▆▃▅▄▅▄▂▂</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>train/acc</td><td>0.74621</td></tr><tr><td>train/loss</td><td>0.82525</td></tr><tr><td>train/lr</td><td>0.001</td></tr><tr><td>val/acc</td><td>0.45946</td></tr><tr><td>val/loss</td><td>2.16453</td></tr></table><br/></div></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">test: ProjectCNN_v22</strong> at: <a href='https://wandb.ai/davidecrociati/assignments%20part%20two/runs/bou9vr94' target=\"_blank\">https://wandb.ai/davidecrociati/assignments%20part%20two/runs/bou9vr94</a><br/> View project at: <a href='https://wandb.ai/davidecrociati/assignments%20part%20two' target=\"_blank\">https://wandb.ai/davidecrociati/assignments%20part%20two</a><br/>Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Find logs at: <code>.\\wandb\\run-20240806_165833-bou9vr94\\logs</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information."
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Best val acc = 0.459\n"
          ]
        },
        {
          "ename": "",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31mSi è verificato un arresto anomalo del Kernel durante l'esecuzione del codice nella cella attiva o in una cella precedente. \n",
            "\u001b[1;31mEsaminare il codice nelle celle per identificare una possibile causa dell'errore. \n",
            "\u001b[1;31mPer altre informazioni, fare clic<a href='https://aka.ms/vscodeJupyterKernelCrash'>qui</a>. \n",
            "\u001b[1;31mPer ulteriori dettagli, visualizzare Jupyter <a href='command:jupyter.viewOutput'>log</a>."
          ]
        }
      ],
      "source": [
        "cfg[\"model\"] = \"v2\"\n",
        "model = ProjectCNN_v2(n_classes=len(classes))\n",
        "if next(model.parameters()).device != device:\n",
        "    model.to(device)\n",
        "summary(\n",
        "    model,\n",
        "    input_size=(3, cfg[\"crop_size\"], cfg[\"crop_size\"])\n",
        ")\n",
        "cfg[\"scheduled\"]=False\n",
        "cfg[\"run_name\"] = f\"{cfg['model']}\" + (\"_scheduled2\" if cfg[\"scheduled\"] else \"2\")\n",
        "\n",
        "# Wandb key: 3f0834114b4b33656e70323616fa377c30c83542\n",
        "\n",
        "trainer = Trainer(\n",
        "    model,\n",
        "    loader_train_augm,\n",
        "    loader_val_augm,\n",
        "    loader_test_augm,\n",
        "    device,\n",
        "    num_classes=len(classes.keys())\n",
        ")\n",
        "trainer.train()\n",
        "print(f\"Best val acc = {trainer.best_acc:.3f}\")\n",
        "# Log results\n",
        "trainer.save_model_params(cfg, data_transforms)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### V3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eHixncdu-SuG"
      },
      "outputs": [],
      "source": [
        "class ResidualBlock(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, stride=1):\n",
        "        super(ResidualBlock, self).__init__()\n",
        "        \n",
        "        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=stride, padding=1)\n",
        "        self.relu = nn.ReLU(inplace=True) \n",
        "        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, stride=1, padding=1)\n",
        "        \n",
        "        # Shortcut connection (skip connection) to add the input to the output of convolutions\n",
        "        self.shortcut = nn.Sequential()\n",
        "        if stride != 1 or in_channels != out_channels:\n",
        "            # If the dimensions change, adjust the shortcut path, in order to maintain consistency in the feature maps\n",
        "            self.shortcut = nn.Sequential(\n",
        "                nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=stride, padding=0) # 1x1 convolution to match dimensions\n",
        "            )\n",
        "    \n",
        "    def forward(self, x):\n",
        "        out = self.relu(self.conv1(x))\n",
        "        out = self.conv2(out)\n",
        "        out += self.shortcut(x)\n",
        "        out = self.relu(out)\n",
        "        return out\n",
        "\n",
        "class ProjectCNN_v3(nn.Module):\n",
        "    def __init__(self, n_classes):\n",
        "        super(ProjectCNN_v3, self).__init__()\n",
        "        \n",
        "        # Residual blocks\n",
        "        self.layer1 = nn.Sequential(\n",
        "            nn.Conv2d(3, 32, kernel_size=3, stride=1, padding=1),  \n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2, padding=0)  \n",
        "        )\n",
        "        self.layer2 = nn.Sequential(\n",
        "            ResidualBlock(32, 64),  \n",
        "            nn.MaxPool2d(kernel_size=2, stride=2, padding=0)  \n",
        "        )\n",
        "        self.layer3 = nn.Sequential(\n",
        "            ResidualBlock(64, 128),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2, padding=0)\n",
        "        )\n",
        "        self.layer4 = nn.Sequential(\n",
        "            ResidualBlock(128, 256),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2, padding=0)\n",
        "        )\n",
        "        self.layer5 = nn.Sequential(\n",
        "            ResidualBlock(256, 512),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2, padding=0)\n",
        "        )\n",
        "        \n",
        "        # FC layers\n",
        "        self.fc1 = nn.Linear(512 * 7 * 7, 1024)  \n",
        "        self.fc2 = nn.Linear(1024, 512)  \n",
        "        self.fc3 = nn.Linear(512, n_classes)  \n",
        "    \n",
        "    def forward(self, x):\n",
        "        x = self.layer1(x)  \n",
        "        x = self.layer2(x)  \n",
        "        x = self.layer3(x)  \n",
        "        x = self.layer4(x)  \n",
        "        x = self.layer5(x)  \n",
        "        x = x.view(x.size(0), -1)  \n",
        "        x = F.relu(self.fc1(x))  \n",
        "        x = F.relu(self.fc2(x))  \n",
        "        x = self.fc3(x)  \n",
        "        return x\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "----------------------------------------------------------------\n",
            "        Layer (type)               Output Shape         Param #\n",
            "================================================================\n",
            "            Conv2d-1         [-1, 32, 224, 224]             896\n",
            "         MaxPool2d-2         [-1, 32, 112, 112]               0\n",
            "            Conv2d-3         [-1, 64, 112, 112]          18,496\n",
            "         MaxPool2d-4           [-1, 64, 56, 56]               0\n",
            "            Linear-5                  [-1, 128]      25,690,240\n",
            "            Linear-6                   [-1, 43]           5,547\n",
            "================================================================\n",
            "Total params: 25,715,179\n",
            "Trainable params: 25,715,179\n",
            "Non-trainable params: 0\n",
            "----------------------------------------------------------------\n",
            "Input size (MB): 0.57\n",
            "Forward/backward pass size (MB): 22.97\n",
            "Params size (MB): 98.10\n",
            "Estimated Total Size (MB): 121.64\n",
            "----------------------------------------------------------------\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "a9e5e7309c2f4728a7840630bb810b80",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.011111111111111112, max=1.0…"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.17.5"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>c:\\Users\\Davide\\Documents\\GitHub\\IPCV-assignments\\part_two\\pelle\\wandb\\run-20240806_122214-96oivgvl</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/davidecrociati/assignments%20part%20two/runs/96oivgvl' target=\"_blank\">earthy-vortex-15</a></strong> to <a href='https://wandb.ai/davidecrociati/assignments%20part%20two' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/davidecrociati/assignments%20part%20two' target=\"_blank\">https://wandb.ai/davidecrociati/assignments%20part%20two</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/davidecrociati/assignments%20part%20two/runs/96oivgvl' target=\"_blank\">https://wandb.ai/davidecrociati/assignments%20part%20two/runs/96oivgvl</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "470d1a7aaab64d058a813c2ab519a384",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Epoch:   0%|          | 0/10 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "cea385c71dcd48b3bff5531e1a094d84",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "VBox(children=(Label(value='0.006 MB of 0.007 MB uploaded\\r'), FloatProgress(value=0.962498175981322, max=1.0)…"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<style>\n",
              "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
              "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
              "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
              "    </style>\n",
              "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>train/acc</td><td>▁▄▄▅▄▆▆▇██</td></tr><tr><td>train/loss</td><td>█▆▇▅▅▅▅▃▄▄▄▄▄▄▅▄▄▄▂▃▃▄▅▁▅▂▂▃▄▂▃▂▇▁▃▁▁▂▃▃</td></tr><tr><td>train/lr</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val/acc</td><td>▁▆▅▇▂▇█▇▇▇</td></tr><tr><td>val/loss</td><td>█▃▅▂▃▂▁▂▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>train/acc</td><td>0.49205</td></tr><tr><td>train/loss</td><td>1.58563</td></tr><tr><td>train/lr</td><td>0.001</td></tr><tr><td>val/acc</td><td>0.33784</td></tr><tr><td>val/loss</td><td>2.31779</td></tr></table><br/></div></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">earthy-vortex-15</strong> at: <a href='https://wandb.ai/davidecrociati/assignments%20part%20two/runs/96oivgvl' target=\"_blank\">https://wandb.ai/davidecrociati/assignments%20part%20two/runs/96oivgvl</a><br/> View project at: <a href='https://wandb.ai/davidecrociati/assignments%20part%20two' target=\"_blank\">https://wandb.ai/davidecrociati/assignments%20part%20two</a><br/>Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Find logs at: <code>.\\wandb\\run-20240806_122214-96oivgvl\\logs</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information."
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Best val acc = 0.355\n"
          ]
        }
      ],
      "source": [
        "cfg[\"model\"] = \"v3\"\n",
        "model = ProjectCNN_v1(n_classes=len(classes))\n",
        "if next(model.parameters()).device != device:\n",
        "    model.to(device)\n",
        "summary(\n",
        "    model,\n",
        "    input_size=(3, cfg[\"crop_size\"], cfg[\"crop_size\"])\n",
        ")\n",
        "cfg[\"scheduled\"]=False\n",
        "cfg[\"run_name\"] = f\"{cfg['model']}\" + \"_scheduled\" if cfg[\"scheduled\"] else \"\"\n",
        "\n",
        "# Wandb key: 3f0834114b4b33656e70323616fa377c30c83542\n",
        "\n",
        "trainer = Trainer(\n",
        "    model,\n",
        "    loader_train,\n",
        "    loader_val,\n",
        "    loader_test,\n",
        "    device,\n",
        "    num_classes=len(classes.keys())\n",
        ")\n",
        "trainer.train()\n",
        "print(f\"Best val acc = {trainer.best_acc:.3f}\")\n",
        "# Log results\n",
        "trainer.save_model_params(cfg, data_transforms)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### V4"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gr9BKlcL-SuG"
      },
      "outputs": [],
      "source": [
        "class ResidualBlock(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, stride=1):\n",
        "        super(ResidualBlock, self).__init__()\n",
        "        \n",
        "        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=stride, padding=1)\n",
        "        self.bn1 = nn.BatchNorm2d(out_channels) # here\n",
        "        self.relu = nn.ReLU(inplace=True)  \n",
        "        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, stride=1, padding=1)\n",
        "        self.bn2 = nn.BatchNorm2d(out_channels) # here\n",
        "        \n",
        "        self.shortcut = nn.Sequential()\n",
        "        if stride != 1 or in_channels != out_channels:\n",
        "            self.shortcut = nn.Sequential(\n",
        "                nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=stride, padding=0),  \n",
        "                nn.BatchNorm2d(out_channels)    # here\n",
        "            )\n",
        "    \n",
        "    def forward(self, x):\n",
        "        out = self.relu(self.bn1(self.conv1(x)))\n",
        "        out = self.bn2(self.conv2(out))\n",
        "        out += self.shortcut(x)\n",
        "        out = self.relu(out)\n",
        "        return out\n",
        "\n",
        "class ProjectCNN_v4(nn.Module):\n",
        "    def __init__(self, n_classes):\n",
        "        super(ProjectCNN_v4, self).__init__()\n",
        "        \n",
        "        # First layer\n",
        "        self.layer1 = nn.Sequential(\n",
        "            nn.Conv2d(3, 32, kernel_size=3, stride=1, padding=1), \n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2, padding=0) \n",
        "        )\n",
        "        \n",
        "        # Residual blocks\n",
        "        self.layer2 = nn.Sequential(\n",
        "            ResidualBlock(32, 64), \n",
        "            nn.MaxPool2d(kernel_size=2, stride=2, padding=0) \n",
        "        )\n",
        "        self.layer3 = nn.Sequential(\n",
        "            ResidualBlock(64, 128), \n",
        "            nn.MaxPool2d(kernel_size=2, stride=2, padding=0)\n",
        "        )\n",
        "        self.layer4 = nn.Sequential(\n",
        "            ResidualBlock(128, 256), \n",
        "            nn.MaxPool2d(kernel_size=2, stride=2, padding=0)\n",
        "        )\n",
        "        self.layer5 = nn.Sequential(\n",
        "            ResidualBlock(256, 512), \n",
        "            nn.MaxPool2d(kernel_size=2, stride=2, padding=0)\n",
        "        )\n",
        "        \n",
        "        # FC layers\n",
        "        self.fcs = nn.Sequential(\n",
        "            nn.Linear(512 * 7 * 7, 1024),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Linear(1024, 512),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Linear(512, n_classes)\n",
        "        )\n",
        "        # self.fc1 = nn.Linear(512 * 7 * 7, 1024)  \n",
        "        # self.fc2 = nn.Linear(1024, 512)  \n",
        "        # self.fc3 = nn.Linear(512, n_classes)  \n",
        "    \n",
        "    def forward(self, x):\n",
        "        x = self.layer1(x)  \n",
        "        x = self.layer2(x)  \n",
        "        x = self.layer3(x)  \n",
        "        x = self.layer4(x)  \n",
        "        x = self.layer5(x)  \n",
        "        x = x.view(x.size(0), -1)  \n",
        "        # x = F.relu(self.fc1(x))  \n",
        "        # x = F.relu(self.fc2(x))  \n",
        "        # x = self.fc3(x)  \n",
        "        x = self.fcs(x)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "----------------------------------------------------------------\n",
            "        Layer (type)               Output Shape         Param #\n",
            "================================================================\n",
            "            Conv2d-1         [-1, 32, 224, 224]             896\n",
            "         MaxPool2d-2         [-1, 32, 112, 112]               0\n",
            "            Conv2d-3         [-1, 64, 112, 112]          18,496\n",
            "         MaxPool2d-4           [-1, 64, 56, 56]               0\n",
            "            Linear-5                  [-1, 128]      25,690,240\n",
            "            Linear-6                   [-1, 43]           5,547\n",
            "================================================================\n",
            "Total params: 25,715,179\n",
            "Trainable params: 25,715,179\n",
            "Non-trainable params: 0\n",
            "----------------------------------------------------------------\n",
            "Input size (MB): 0.57\n",
            "Forward/backward pass size (MB): 22.97\n",
            "Params size (MB): 98.10\n",
            "Estimated Total Size (MB): 121.64\n",
            "----------------------------------------------------------------\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "5b0661a10e9842518b56f9d1e9ba7528",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.011111111111111112, max=1.0…"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.17.5"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>c:\\Users\\Davide\\Documents\\GitHub\\IPCV-assignments\\part_two\\pelle\\wandb\\run-20240806_122655-2r4yut48</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/davidecrociati/assignments%20part%20two/runs/2r4yut48' target=\"_blank\">usual-puddle-16</a></strong> to <a href='https://wandb.ai/davidecrociati/assignments%20part%20two' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/davidecrociati/assignments%20part%20two' target=\"_blank\">https://wandb.ai/davidecrociati/assignments%20part%20two</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/davidecrociati/assignments%20part%20two/runs/2r4yut48' target=\"_blank\">https://wandb.ai/davidecrociati/assignments%20part%20two/runs/2r4yut48</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "d48923490f9c4658a19b5e401456554d",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Epoch:   0%|          | 0/10 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "d7104297d4934e12bf67d372b26197a5",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "VBox(children=(Label(value='0.001 MB of 0.007 MB uploaded\\r'), FloatProgress(value=0.21193986279375274, max=1.…"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<style>\n",
              "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
              "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
              "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
              "    </style>\n",
              "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>train/acc</td><td>▁▂▃▄▅▆▆▇▆█</td></tr><tr><td>train/loss</td><td>██▇▆▆▇▅▄▄▄▆▄▄▆▅▃▄▃█▂▃▃▂▄▃▂▅▃▂▃▂▁▂▆▄▁▃▂▁▁</td></tr><tr><td>train/lr</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val/acc</td><td>▁▁▃▄▅▅▆▇▅█</td></tr><tr><td>val/loss</td><td>█▇▅▃▂▃▃▂▅▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>train/acc</td><td>0.48409</td></tr><tr><td>train/loss</td><td>1.65224</td></tr><tr><td>train/lr</td><td>0.001</td></tr><tr><td>val/acc</td><td>0.36824</td></tr><tr><td>val/loss</td><td>2.47052</td></tr></table><br/></div></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">usual-puddle-16</strong> at: <a href='https://wandb.ai/davidecrociati/assignments%20part%20two/runs/2r4yut48' target=\"_blank\">https://wandb.ai/davidecrociati/assignments%20part%20two/runs/2r4yut48</a><br/> View project at: <a href='https://wandb.ai/davidecrociati/assignments%20part%20two' target=\"_blank\">https://wandb.ai/davidecrociati/assignments%20part%20two</a><br/>Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Find logs at: <code>.\\wandb\\run-20240806_122655-2r4yut48\\logs</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information."
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Best val acc = 0.368\n"
          ]
        }
      ],
      "source": [
        "cfg[\"model\"] = \"v4\"\n",
        "model = ProjectCNN_v1(n_classes=len(classes))\n",
        "if next(model.parameters()).device != device:\n",
        "    model.to(device)\n",
        "summary(\n",
        "    model,\n",
        "    input_size=(3, cfg[\"crop_size\"], cfg[\"crop_size\"])\n",
        ")\n",
        "cfg[\"scheduled\"]=False\n",
        "cfg[\"run_name\"] = f\"{cfg['model']}\" + \"_scheduled\" if cfg[\"scheduled\"] else \"\"\n",
        "\n",
        "# Wandb key: 3f0834114b4b33656e70323616fa377c30c83542\n",
        "\n",
        "trainer = Trainer(\n",
        "    model,\n",
        "    loader_train,\n",
        "    loader_val,\n",
        "    loader_test,\n",
        "    device,\n",
        "    num_classes=len(classes.keys())\n",
        ")\n",
        "trainer.train()\n",
        "print(f\"Best val acc = {trainer.best_acc:.3f}\")\n",
        "# Log results\n",
        "trainer.save_model_params(cfg, data_transforms)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gkWEqSPoUIL3"
      },
      "source": [
        "## Part 2: fine-tune an existing network\n",
        "\n",
        "Your goal is to fine-tune a pretrained **ResNet-18** model on `GroceryStoreDataset`. Use the implementation provided by PyTorch, do not implement it yourselves! (i.e. exactly what you **could not** do in the first part of the assignment). Specifically, you must use the PyTorch ResNet-18 model pretrained on ImageNet-1K (V1). Divide your fine-tuning into two parts:\n",
        "\n",
        "1. First, fine-tune the Resnet-18 with the same training hyperparameters you used for your best model in the first part of the assignment.\n",
        "1. Then, tweak the training hyperparameters in order to increase the accuracy on the validation split of `GroceryStoreDataset`. Justify your choices by analyzing the training plots and/or citing sources that guided you in your decisions (papers, blog posts, YouTube videos, or whatever else you find enlightening). You should consider yourselves satisfied once you obtain a classification accuracy on the **validation** split **between 80 and 90%**."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jX48RzTypUuO"
      },
      "source": [
        "### 1) Fine-tune the Resnet-18 with the same training hyperparameters you used for your best model in the first part of the assignment."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Vtb4wn_vX8mN"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Downloading: \"https://download.pytorch.org/models/resnet18-f37072fd.pth\" to C:\\Users\\Davide/.cache\\torch\\hub\\checkpoints\\resnet18-f37072fd.pth\n",
            "100%|██████████| 44.7M/44.7M [00:12<00:00, 3.66MB/s]\n"
          ]
        }
      ],
      "source": [
        "# Pretrained ResNet-18 on ImageNet-1K (V1)\n",
        "pt_resnet18 = resnet18(weights=ResNet18_Weights.IMAGENET1K_V1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dgE0KRTj6JSd"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.17.5"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>c:\\Users\\Davide\\Documents\\GitHub\\IPCV-assignments\\part_two\\pelle\\wandb\\run-20240806_123151-09csq8pz</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/davidecrociati/assignments%20part%20two/runs/09csq8pz' target=\"_blank\">ResNet-18_pretrained_scratch-training</a></strong> to <a href='https://wandb.ai/davidecrociati/assignments%20part%20two' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/davidecrociati/assignments%20part%20two' target=\"_blank\">https://wandb.ai/davidecrociati/assignments%20part%20two</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/davidecrociati/assignments%20part%20two/runs/09csq8pz' target=\"_blank\">https://wandb.ai/davidecrociati/assignments%20part%20two/runs/09csq8pz</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "6b00e18f05754d95a9265b50c726be4c",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Epoch:   0%|          | 0/10 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "afccd9d8755f41fc8ffbf2c6e8d00af1",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "VBox(children=(Label(value='0.001 MB of 0.007 MB uploaded\\r'), FloatProgress(value=0.21057971014492755, max=1.…"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<style>\n",
              "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
              "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
              "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
              "    </style>\n",
              "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>train/acc</td><td>▁▄▅▆▇▇▇███</td></tr><tr><td>train/loss</td><td>█▅▅▃▃▃▄▂▂▃▃▂▂▂▂▂▃▂▂▁▂▂▂▃▁▁▂▂▂▂▂▁▂▂▂▁▂▂▂▃</td></tr><tr><td>train/lr</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val/acc</td><td>▁▄▅▆▇▆█▇▇█</td></tr><tr><td>val/loss</td><td>█▅▃▂▂▂▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>train/acc</td><td>0.82197</td></tr><tr><td>train/loss</td><td>0.61639</td></tr><tr><td>train/lr</td><td>0.001</td></tr><tr><td>val/acc</td><td>0.70946</td></tr><tr><td>val/loss</td><td>0.94376</td></tr></table><br/></div></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">ResNet-18_pretrained_scratch-training</strong> at: <a href='https://wandb.ai/davidecrociati/assignments%20part%20two/runs/09csq8pz' target=\"_blank\">https://wandb.ai/davidecrociati/assignments%20part%20two/runs/09csq8pz</a><br/> View project at: <a href='https://wandb.ai/davidecrociati/assignments%20part%20two' target=\"_blank\">https://wandb.ai/davidecrociati/assignments%20part%20two</a><br/>Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Find logs at: <code>.\\wandb\\run-20240806_123151-09csq8pz\\logs</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information."
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "## Freeze the pre-trained layers ##\n",
        "for param in pt_resnet18.parameters():\n",
        "    param.requires_grad = False\n",
        "\n",
        "## Modify the last layer ##\n",
        "pt_resnet18.fc = torch.nn.Linear(pt_resnet18.fc.in_features, len(classes.keys()))\n",
        "\n",
        "## First train\n",
        "cfg[\"run_name\"] = \"ResNet-18_pretrained_scratch-training\"\n",
        "## cfg[\"num_epochs\"] = 5\n",
        "trainer = Trainer(\n",
        "    pt_resnet18,\n",
        "    loader_train,\n",
        "    loader_val,\n",
        "    loader_test,\n",
        "    device,\n",
        "    num_classes=len(classes.keys())\n",
        ")\n",
        "trainer.train()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-w4tkV0A5A3O"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Best val acc (intermediate) = 0.723\n"
          ]
        }
      ],
      "source": [
        "print(f\"Best val acc (intermediate) = {trainer.best_acc:.3f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FRWEMxgM1KuQ"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "a0481bfcc2b44e55a70f1e4d086fc78e",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.011277777777932999, max=1.0…"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.17.5"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>c:\\Users\\Davide\\Documents\\GitHub\\IPCV-assignments\\part_two\\pelle\\wandb\\run-20240806_123700-2u7t22qw</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/davidecrociati/assignments%20part%20two/runs/2u7t22qw' target=\"_blank\">ResNet-18_pretrained_fine-tuning</a></strong> to <a href='https://wandb.ai/davidecrociati/assignments%20part%20two' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/davidecrociati/assignments%20part%20two' target=\"_blank\">https://wandb.ai/davidecrociati/assignments%20part%20two</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/davidecrociati/assignments%20part%20two/runs/2u7t22qw' target=\"_blank\">https://wandb.ai/davidecrociati/assignments%20part%20two/runs/2u7t22qw</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "1e8fa2ea1cfa4896a62f79c6bdd21dc9",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Epoch:   0%|          | 0/10 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "44bb44fcc74d4f128c381caa102e40eb",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "VBox(children=(Label(value='0.001 MB of 0.006 MB uploaded\\r'), FloatProgress(value=0.21885826178641363, max=1.…"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<style>\n",
              "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
              "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
              "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
              "    </style>\n",
              "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>train/acc</td><td>▁▄▅▇▅▇▆▆██</td></tr><tr><td>train/loss</td><td>▇▅█▆▄▃▃▃▄▃▄▂▂▁▂▄▃▃▁▅▃▄▄▁▃▂▁▁▂▆▃▂▂▂▃▁▂▂▂▂</td></tr><tr><td>train/lr</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val/acc</td><td>▁▄▇▇▃▅▇█▆▇</td></tr><tr><td>val/loss</td><td>▇▆▅▃█▅▂▂▅▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>train/acc</td><td>0.97197</td></tr><tr><td>train/loss</td><td>0.09855</td></tr><tr><td>train/lr</td><td>0.0001</td></tr><tr><td>val/acc</td><td>0.86824</td></tr><tr><td>val/loss</td><td>0.40012</td></tr></table><br/></div></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">ResNet-18_pretrained_fine-tuning</strong> at: <a href='https://wandb.ai/davidecrociati/assignments%20part%20two/runs/2u7t22qw' target=\"_blank\">https://wandb.ai/davidecrociati/assignments%20part%20two/runs/2u7t22qw</a><br/> View project at: <a href='https://wandb.ai/davidecrociati/assignments%20part%20two' target=\"_blank\">https://wandb.ai/davidecrociati/assignments%20part%20two</a><br/>Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Find logs at: <code>.\\wandb\\run-20240806_123700-2u7t22qw\\logs</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information."
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "## Load the state of the best accuracy\n",
        "sd = torch.load(f\"ckpts/{cfg['run_name']}.pt\")\n",
        "pt_resnet18.load_state_dict(sd)\n",
        "\n",
        "## Activate learning\n",
        "for param in pt_resnet18.parameters():\n",
        "    param.requires_grad = True\n",
        "\n",
        "## Fine tuning\n",
        "cfg[\"run_name\"] = \"ResNet-18_pretrained_fine-tuning\"\n",
        "cfg[\"lr\"] *= 0.1\n",
        "cfg[\"num_epochs\"] = 10\n",
        "trainer = Trainer(\n",
        "    pt_resnet18,\n",
        "    loader_train,\n",
        "    loader_val,\n",
        "    loader_test,\n",
        "    device,\n",
        "    num_classes=len(classes.keys())\n",
        ")\n",
        "trainer.train()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0PFUF8KP5IOz"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Best val acc (final) = 0.878\n"
          ]
        }
      ],
      "source": [
        "print(f\"Best val acc (final) = {trainer.best_acc:.3f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AJ7yKSjLpfyf"
      },
      "source": [
        "### 2) Tweak the training hyperparameters in order to increase the accuracy on the validation split of `GroceryStoreDataset`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d0TmxpVmpkkx"
      },
      "outputs": [],
      "source": [
        "# You need to do the same but with different cfg\n",
        "cfg_fine_tuning = {\n",
        "    \"resize_size\": 256,\n",
        "    \"crop_size\": 224,\n",
        "    \"batch_size\": 16,\n",
        "    \"num_epochs\": 30,\n",
        "    \"lr\": 1e-3,\n",
        "    \"wd\": 5e-4,\n",
        "    \"step_size\": 5,\n",
        "    'scheduled':True\n",
        "}\n",
        "cfg = cfg_fine_tuning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7VQpcnWU14Vf"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.17.5"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>c:\\Users\\Davide\\Documents\\GitHub\\IPCV-assignments\\part_two\\pelle\\wandb\\run-20240806_124521-xcrnntez</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/davidecrociati/assignments%20part%20two/runs/xcrnntez' target=\"_blank\">ResNet-18_pretrained_scratch-training_2</a></strong> to <a href='https://wandb.ai/davidecrociati/assignments%20part%20two' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/davidecrociati/assignments%20part%20two' target=\"_blank\">https://wandb.ai/davidecrociati/assignments%20part%20two</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/davidecrociati/assignments%20part%20two/runs/xcrnntez' target=\"_blank\">https://wandb.ai/davidecrociati/assignments%20part%20two/runs/xcrnntez</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "b8f4b0f4ddd94d569d27fde322872f41",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Epoch:   0%|          | 0/30 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[30], line 19\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;66;03m# cfg[\"num_epochs\"] = 5\u001b[39;00m\n\u001b[0;32m     11\u001b[0m trainer \u001b[38;5;241m=\u001b[39m Trainer(\n\u001b[0;32m     12\u001b[0m     pt_resnet18,\n\u001b[0;32m     13\u001b[0m     loader_train,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     17\u001b[0m     num_classes\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mlen\u001b[39m(classes\u001b[38;5;241m.\u001b[39mkeys())\n\u001b[0;32m     18\u001b[0m )\n\u001b[1;32m---> 19\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     20\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBest val acc (intermediate) = \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrainer\u001b[38;5;241m.\u001b[39mbest_acc\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.3f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
            "Cell \u001b[1;32mIn[12], line 59\u001b[0m, in \u001b[0;36mTrainer.train\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     55\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlogfn({\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain/lr\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscheduler\u001b[38;5;241m.\u001b[39mget_last_lr()[\u001b[38;5;241m0\u001b[39m]})\n\u001b[0;32m     57\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstep \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m---> 59\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meval\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtrain\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     60\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39meval(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mval\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     62\u001b[0m wandb\u001b[38;5;241m.\u001b[39mfinish()\n",
            "File \u001b[1;32mc:\\Users\\Davide\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\utils\\_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    112\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[0;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m    114\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[1;32m--> 115\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
            "Cell \u001b[1;32mIn[12], line 87\u001b[0m, in \u001b[0;36mTrainer.eval\u001b[1;34m(self, split)\u001b[0m\n\u001b[0;32m     84\u001b[0m acc \u001b[38;5;241m=\u001b[39m Accuracy(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmulticlass\u001b[39m\u001b[38;5;124m\"\u001b[39m, num_classes\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_classes)\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[0;32m     86\u001b[0m losses \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m---> 87\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m imgs, labels \u001b[38;5;129;01min\u001b[39;00m loader:\n\u001b[0;32m     88\u001b[0m     imgs \u001b[38;5;241m=\u001b[39m imgs\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[0;32m     89\u001b[0m     labels \u001b[38;5;241m=\u001b[39m labels\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice)\n",
            "File \u001b[1;32mc:\\Users\\Davide\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:631\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    628\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    629\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    630\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 631\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    632\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    633\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    634\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    635\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
            "File \u001b[1;32mc:\\Users\\Davide\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:675\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    673\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    674\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m--> 675\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    676\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[0;32m    677\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
            "File \u001b[1;32mc:\\Users\\Davide\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:51\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     49\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[0;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 51\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
            "File \u001b[1;32mc:\\Users\\Davide\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:51\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     49\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[0;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 51\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
            "Cell \u001b[1;32mIn[3], line 20\u001b[0m, in \u001b[0;36mGroceryStoreDataset.__getitem__\u001b[1;34m(self, idx)\u001b[0m\n\u001b[0;32m     17\u001b[0m label \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlabels[idx]\n\u001b[0;32m     19\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransform:\n\u001b[1;32m---> 20\u001b[0m     img \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     22\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m img, label\n",
            "File \u001b[1;32mc:\\Users\\Davide\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torchvision\\transforms\\transforms.py:95\u001b[0m, in \u001b[0;36mCompose.__call__\u001b[1;34m(self, img)\u001b[0m\n\u001b[0;32m     93\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, img):\n\u001b[0;32m     94\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransforms:\n\u001b[1;32m---> 95\u001b[0m         img \u001b[38;5;241m=\u001b[39m \u001b[43mt\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     96\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m img\n",
            "File \u001b[1;32mc:\\Users\\Davide\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
            "File \u001b[1;32mc:\\Users\\Davide\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
            "File \u001b[1;32mc:\\Users\\Davide\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torchvision\\transforms\\transforms.py:973\u001b[0m, in \u001b[0;36mRandomResizedCrop.forward\u001b[1;34m(self, img)\u001b[0m\n\u001b[0;32m    965\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    966\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[0;32m    967\u001b[0m \u001b[38;5;124;03m    img (PIL Image or Tensor): Image to be cropped and resized.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    970\u001b[0m \u001b[38;5;124;03m    PIL Image or Tensor: Randomly cropped and resized image.\u001b[39;00m\n\u001b[0;32m    971\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    972\u001b[0m i, j, h, w \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_params(img, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscale, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mratio)\n\u001b[1;32m--> 973\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresized_crop\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mh\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mw\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msize\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minterpolation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mantialias\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mantialias\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32mc:\\Users\\Davide\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torchvision\\transforms\\functional.py:640\u001b[0m, in \u001b[0;36mresized_crop\u001b[1;34m(img, top, left, height, width, size, interpolation, antialias)\u001b[0m\n\u001b[0;32m    638\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mjit\u001b[38;5;241m.\u001b[39mis_scripting() \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mjit\u001b[38;5;241m.\u001b[39mis_tracing():\n\u001b[0;32m    639\u001b[0m     _log_api_usage_once(resized_crop)\n\u001b[1;32m--> 640\u001b[0m img \u001b[38;5;241m=\u001b[39m \u001b[43mcrop\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mleft\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mheight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwidth\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    641\u001b[0m img \u001b[38;5;241m=\u001b[39m resize(img, size, interpolation, antialias\u001b[38;5;241m=\u001b[39mantialias)\n\u001b[0;32m    642\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m img\n",
            "File \u001b[1;32mc:\\Users\\Davide\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torchvision\\transforms\\functional.py:542\u001b[0m, in \u001b[0;36mcrop\u001b[1;34m(img, top, left, height, width)\u001b[0m\n\u001b[0;32m    540\u001b[0m     _log_api_usage_once(crop)\n\u001b[0;32m    541\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(img, torch\u001b[38;5;241m.\u001b[39mTensor):\n\u001b[1;32m--> 542\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF_pil\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcrop\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mleft\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mheight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwidth\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    544\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m F_t\u001b[38;5;241m.\u001b[39mcrop(img, top, left, height, width)\n",
            "File \u001b[1;32mc:\\Users\\Davide\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torchvision\\transforms\\_functional_pil.py:235\u001b[0m, in \u001b[0;36mcrop\u001b[1;34m(img, top, left, height, width)\u001b[0m\n\u001b[0;32m    232\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m _is_pil_image(img):\n\u001b[0;32m    233\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mimg should be PIL Image. Got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(img)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 235\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mimg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcrop\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[43mleft\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mleft\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mwidth\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mheight\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32mc:\\Users\\Davide\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\PIL\\Image.py:1242\u001b[0m, in \u001b[0;36mImage.crop\u001b[1;34m(self, box)\u001b[0m\n\u001b[0;32m   1239\u001b[0m     msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCoordinate \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlower\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m is less than \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mupper\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1240\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(msg)\n\u001b[1;32m-> 1242\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1243\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_new(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_crop(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mim, box))\n",
            "File \u001b[1;32mc:\\Users\\Davide\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\PIL\\ImageFile.py:291\u001b[0m, in \u001b[0;36mImageFile.load\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    288\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m(msg)\n\u001b[0;32m    290\u001b[0m b \u001b[38;5;241m=\u001b[39m b \u001b[38;5;241m+\u001b[39m s\n\u001b[1;32m--> 291\u001b[0m n, err_code \u001b[38;5;241m=\u001b[39m \u001b[43mdecoder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    292\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m n \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m    293\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "## Freeze the pre-trained layers ##\n",
        "for param in pt_resnet18.parameters():\n",
        "    param.requires_grad = False\n",
        "\n",
        "## Modify the last layer ##\n",
        "pt_resnet18.fc = torch.nn.Linear(pt_resnet18.fc.in_features, len(classes.keys()))\n",
        "\n",
        "# First train\n",
        "cfg[\"run_name\"] = \"ResNet-18_pretrained_scratch-training_2\"\n",
        "# cfg[\"num_epochs\"] = 5\n",
        "trainer = Trainer(\n",
        "    pt_resnet18,\n",
        "    loader_train,\n",
        "    loader_val,\n",
        "    loader_test,\n",
        "    device,\n",
        "    num_classes=len(classes.keys())\n",
        ")\n",
        "trainer.train()\n",
        "print(f\"Best val acc (intermediate) = {trainer.best_acc:.3f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U4wM4IVL19IU"
      },
      "outputs": [],
      "source": [
        "# Load the state of the best accuracy\n",
        "sd = torch.load(f\"ckpts/{cfg['run_name']}.pt\")\n",
        "pt_resnet18.load_state_dict(sd)\n",
        "\n",
        "## Activate learning\n",
        "for param in pt_resnet18.parameters():\n",
        "    param.requires_grad = True\n",
        "\n",
        "# Fine tuning\n",
        "cfg[\"run_name\"] = \"ResNet-18_pretrained_fine-tuning_2\"\n",
        "cfg[\"lr\"] *= 0.1\n",
        "cfg[\"num_epochs\"] = 10\n",
        "trainer = Trainer(\n",
        "    pt_resnet18,\n",
        "    loader_train,\n",
        "    loader_val,\n",
        "    loader_test,\n",
        "    device,\n",
        "    num_classes=len(classes.keys())\n",
        ")\n",
        "trainer.train()\n",
        "print(f\"Best val acc (final) = {trainer.best_acc:.3f}\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "IEaLcecbH8qK",
        "32kWjbqzH8qM",
        "2F1cwsSnH8qO",
        "a1e0PtlZH8qQ",
        "1de1sRClH8qS"
      ],
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.11"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "040fe1e1dd904d2aaf028534b512ccc1": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "075fc403833a4156abeeffa2ac2df6c2": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "22daac61aec74a72ba168bbaa6016142": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2347c18b708f4d3a9220066e4d079c79": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "47bda30d90c74e479bf61283872b1619": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_22daac61aec74a72ba168bbaa6016142",
            "placeholder": "​",
            "style": "IPY_MODEL_6c69eba85b2e44f38f50f03d33151ed5",
            "value": "Epoch:   0%"
          }
        },
        "529c6eb1c4554d419bfa1b1fe0097af9": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_47bda30d90c74e479bf61283872b1619",
              "IPY_MODEL_9cd1bf52e59f476b8ce040cd598c74ad",
              "IPY_MODEL_a2187fdbc26c44999b18ef1f0c970235"
            ],
            "layout": "IPY_MODEL_6bdaadac256b4e888b09407893931644"
          }
        },
        "6bdaadac256b4e888b09407893931644": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6c69eba85b2e44f38f50f03d33151ed5": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "8bee60b8c90b480e9424e121ae601b87": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "9cd1bf52e59f476b8ce040cd598c74ad": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_075fc403833a4156abeeffa2ac2df6c2",
            "max": 50,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_2347c18b708f4d3a9220066e4d079c79",
            "value": 0
          }
        },
        "a2187fdbc26c44999b18ef1f0c970235": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_040fe1e1dd904d2aaf028534b512ccc1",
            "placeholder": "​",
            "style": "IPY_MODEL_8bee60b8c90b480e9424e121ae601b87",
            "value": " 0/50 [00:00&lt;?, ?it/s]"
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
